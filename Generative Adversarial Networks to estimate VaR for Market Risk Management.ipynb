{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"GANVaR.ipynb","version":"0.3.2","provenance":[]},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"L9JitBk6dK8f","colab_type":"text"},"source":["# Using Bidirectional Generative Adversarial Networks to estimate Value-at-Risk for Market Risk Management\n","\n","---\n","\n","We will explore the use of Bidirectional Generative Adversarial Networks (BiGAN) for market risk management: Estimation of portfolio risk measures such as Value-at-Risk (VaR). Generative Adversarial Networks (GAN) allow us to implicitly maximize the likelihood of complex distributions thereby allowing us to generate samples from such distributions - the key point here is the implicit maximum likelihood estimation principle whereby we do not specify what this complex distribution is parameterized as. Dealing with high dimensional data potentially coming from a complex distribution is a key aspect to market risk management among many other financial services use cases. GAN, specifically BiGAN for the purpose of this paper, will allow us to deal with potentially complex financial services data such that we do not have to explicitly specify a distribution such as a multidimensional Gaussian distribution.\n","\n","## Market Risk Management: Value-at-Risk (VaR)\n","\n","---\n","\n","VaR is a measure of portfolio risk. For instance, a 1% VaR of -5% means that there is a 1% chance of earning a portfolio return of less than -5%. Think of it as a (lower) percentile or quantile of a portfolio returns distribution, i.e., we are concerned about the tail risk — the small chance of losing a remarkably large portfolio value. Such a large loss is funded by our own funds, i.e., capital which is an expensive source of funding compared to other peoples’ funds, i.e., debt. Therefore the estimation of VaR and similar market risk management measures inform banks and insurance firms with regards to the levels of capital they need to hold in order to have a buffer against unexpected downturns — market risk.\n","\n","For our purpose we can fetch 5 stocks: Apple, Google, Microsoft, Intel and Box. We use a daily frequency for our data for the year 2018. We use the stock's daily closing prices to compute the continuously compounded returns: $\\log\\left(\\frac{V_{t+1}}{V_{t}}\\right) = \\log(V_{t+1}) - \\log(V_{t})$.\n","\n","\n","\n","Let's estimate the expected returns vector, volatilities vector, correlation and variance-covariance matrices. The variance-covariance matrix is recovered from the estimated volatilities vector and correlation matrix: $\\Omega = C \\odot \\sigma \\sigma^{T}$ where $\\odot$ is the Hadamard product, $C \\in \\mathbb{R}^{5 \\times 5}$ and $\\sigma \\in \\mathbb{R}^{5 \\times 1}$. Portfolio volatility is estimated as: $w^{T}\\Omega w$ where $w \\in \\mathbb{R}^{5 \\times 1}$\n","\n","\n","We consider the 3 major methods used in market risk management, specifically for the estimation of VaR. Please note that there are multiple different methods for estimating VaR and other more coherent risk measures such as Conditional Value-at-Risk (CVaR) however we are only considering the few major ones.\n","\n","## VaR: Variance-covariance method\n","\n","---\n","\n","The first one is the variance-covariance method and uses the estimated portfolio volatility $w^{T}\\Omega w$ under the Gaussian assumption to estimate VaR. Let's assume we are attempting to estimate 1% VaR: This means that there is a 1% probability of obtaining a portfolio return of less than the VaR value. Using the variance-covariance approach the calculation is: $\\left[\\left(w^{T}\\Omega w\\right) \\mathcal{N}^{-1}(1\\%)\\right] + w^{T}\\mu$, where $\\mu \\in \\mathbb{R}^{5 \\times 1}$ is the expected returns vector.\n","\n","\n","\n","## VaR: Historical simulation method\n","The second method is a non-parametric approach where we sample with replacement from the historical data to estimate a portfolio returns distribution. The 1% VaR is simply the appropriate quantile from this sampled portfolio returns distribution.\n","\n","\n","\n","## VaR: Monte Carlo method\n","\n","---\n","\n","The third method is Monte Carlo sampling from a multidimensional Gaussian distribution using the aforementioned $mu$ and $\\Omega$ parameters. Finally the 1% VaR is simply the appropriate quantile from this sampled portfolio returns distribution.\n","\n","\n","\n","## VaR: Estimates\n","The VaR estimates from the aforementioned 3 market risk management methods commonly used in banking are as follows:\n","\n","\n","\n","| VaR Method    | 1% VaR | \n","| :------------- |-------------:|\n","| Variance-covariance | -2.87% | \n","| Historical simulation | -3.65%  |\n","| Monte Carlo simulation | -2.63%  |"]},{"cell_type":"markdown","metadata":{"id":"ZEWDSxcWdK8h","colab_type":"text"},"source":["## Bidirectional Generative Adversarial Network (BiGAN)\n","\n","---\n","\n","The 2 main components to a Generative Adversarial Network (GAN) are the generator and the discriminator. These 2 components play an adversarial game against each other. In doing so the generator learns how to create realistic synthetic samples from noise, i.e., the latent space $z$, while the discriminator learns how to distinguish between a real sample and a synthetic sample. \n","\n","BiGAN extends GAN by adding a third component: The encoder, which learns to map from data space $x$ to the latent space $z$. The objective of the generator remains the same while the objective of the discriminator is altered to classify between a real sample and a synthetic sample and additionally between a real encoding, i.e., given by the encoder, and a synthetic encoding, i.e., a sample from the latent space $z$.\n","\n","### Generator\n","\n","---\n","\n","Assume that we have a prior belief on where the latent space $z$ lies: $p_{Z}(z)$. Given a draw from this latent space the generator $G$, a deep learner parameterized by $\\theta_{G}$, outputs a synthetic sample.\n","\n","$$\n","G(z|\\theta_{G}): z \\rightarrow x_{synthetic}\n","$$ \n","\n","### Encoder\n","\n","---\n","\n","This can be shown to be an inverse of the generator. Given a draw from the data space the encoder $E$, a deep learner parameterized by $\\theta_{E}$, outputs a real encoding.\n","\n","$$\n","E(x|\\theta_{E}): x \\rightarrow z\n","$$ \n","\n","### Discriminator\n","\n","---\n","\n","The discriminator $D$ is a deep learner parameterized by $\\theta_{D}$ and it aims to classify if a sample is real or synthetic, i.e., if a sample is from the real data distribution,\n","\n","$$\n","p_{X}(x)\n","$$ \n","\n","or the synthetic data distribution.\n","\n","$$\n","p_{G}(x|z)\n","$$\n","\n","Additionally it aims to classify whether an encoding is real,\n","\n","$$\n","p_{E}(z|x)\n","$$\n","\n","or synthetic.\n","\n","$$\n","p_{Z}(z) \n","$$\n","\n","Let us denote the discriminator $D$ as follows.\n","\n","$$\n","D(\\{x, z\\}|\\theta_{D}): \\{x, z\\} \\rightarrow [0, 1]\n","$$ \n","\n","We assume that the positive examples are real, i.e., $\\{x, E(x|\\theta_{E})\\}$ while the negative examples are synthetic, i.e., $\\{G(z|\\theta_{G}), z\\}$. \n","\n","### Optimal discriminator, encoder and generator\n","\n","---\n","\n","The BiGAN has the following objective function, similar to the GAN.\n","\n","$$\n","\\min_{G(z|\\theta_{G}), E(x|\\theta_{E})} \\max_{D(\\{x, z\\}|\\theta_{D})} V(D(\\{x, z\\}|\\theta_{D}), G(z|\\theta_{G}), E(x|\\theta_{E}))\n","$$\n","\n","\\begin{align*}\n","V(D(\\{x, z\\}|\\theta_{D}), G(z|\\theta_{G}), E(x|\\theta_{E})) &= \\mathbb{E}_{x \\sim p_{X}(x)} \\mathbb{E}_{z \\sim p_{E}(z|x)} \\log\\left[{D(\\{x, z\\}|\\theta_{D})}\\right] + \\mathbb{E}_{z \\sim p_{Z}(z)} \\mathbb{E}_{x \\sim p_{G}(x|z)} \\log\\left[{1-D(\\{x, z\\}|\\theta_{D})}\\right] \\\\\n","&= \\int_{x} p_{X}(x) \\int_{z} p_{E}(z|x) \\log\\left[{D(\\{x, z\\}|\\theta_{D})}\\right] dz dx + \\int_{z} p_{Z}(z) \\int_{x} p_{G}(x|z) \\log\\left[{1 - D(\\{x, z\\}|\\theta_{D})}\\right] dx dz \\\\\n","&= \\int_{\\{x, z\\}} p_{X}(x) p_{E}(z|x) \\log\\left[{D(\\{x, z\\}|\\theta_{D})}\\right] d\\{x, z\\} + \\int_{\\{x, z\\}} p_{Z}(z) p_{G}(x|z) \\log\\left[{1 - D(\\{x, z\\}|\\theta_{D})}\\right] d\\{x, z\\} \\\\\n","&= \\int_{\\omega:=\\{x, z\\}} \\underbrace{p_{EX}(\\omega) \\log\\left[{D(\\omega|\\theta_{D})}\\right] + p_{GZ}(\\omega) \\log\\left[{1 - D(\\omega|\\theta_{D})}\\right]}_{J(D(\\omega|\\theta_{D}))} d\\omega \\\\\n","\\end{align*}\n","\n","Let us take a closer look at the discriminator's objective function for a sample $\\omega$.\n","\n","\\begin{align*}\n","J(D(\\omega|\\theta_{D})) &= p_{EX}(\\omega) \\log{D(\\omega|\\theta_{D})} + p_{GZ}(\\omega) \\log{(1 - D(\\omega|\\theta_{D}))} \\\\\n","\\frac{\\partial J(D(\\omega|\\theta_{D}))}{\\partial D(\\omega|\\theta_{D})} &= \\frac{p_{EX}(\\omega)}{D(\\omega|\\theta_{D})} - \\frac{p_{GZ}(\\omega)}{(1 - D(\\omega|\\theta_{D}))} \\\\\n","0 &= \\frac{p_{EX}(\\omega)}{D^\\ast(\\omega|\\theta_{D^\\ast})} - \\frac{p_{GZ}(\\omega)}{(1 - D^\\ast(\\omega|\\theta_{D^\\ast}))} \\\\\n","p_{EX}(\\omega)(1 - D^\\ast(\\omega|\\theta_{D^\\ast})) &= p_{GZ}(\\omega)D^\\ast(\\omega|\\theta_{D^\\ast}) \\\\\n","p_{EX}(\\omega) - p_{EX}(\\omega)D^\\ast(\\omega|\\theta_{D^\\ast})) &= p_{GZ}(\\omega)D^\\ast(\\omega|\\theta_{D^\\ast}) \\\\\n","p_{GZ}(\\omega)D^\\ast(\\omega|\\theta_{D^\\ast}) + p_{EX}(\\omega)D^\\ast(\\omega|\\theta_{D^\\ast})) &= p_{EX}(\\omega) \\\\\n","D^\\ast(\\omega|\\theta_{D^\\ast}) &= \\frac{p_{EX}(\\omega)}{p_{EX}(\\omega) + p_{GZ}(\\omega)} \n","\\end{align*}\n","\n","We have found the optimal discriminator given a generator and an encoder. Let us focus now on the generator and encoder's objective function which is essentially to minimize the discriminator's objective function.\n","\n","\\begin{align*}\n","J(G(z|\\theta_{G}), E(x|\\theta_{E})) &= \\mathbb{E}_{\\omega \\sim p_{EX}(\\omega)} \\log{D^\\ast(\\omega|\\theta_{D^\\ast})} + \\mathbb{E}_{\\omega \\sim p_{GZ}(\\omega)} \\log{(1 - D^\\ast(\\omega|\\theta_{D^\\ast}))} \\\\\n","&= \\mathbb{E}_{\\omega \\sim p_{EX}(\\omega)} \\log{\\bigg( \\frac{p_{EX}(\\omega)}{p_{EX}(\\omega) + p_{GZ}(\\omega)}} \\bigg) + \\mathbb{E}_{\\omega \\sim p_{GZ}(\\omega)} \\log{\\bigg(1 - \\frac{p_{EX}(\\omega)}{p_{EX}(\\omega) + p_{GZ}(\\omega)}\\bigg)} \\\\\n","&= \\mathbb{E}_{\\omega \\sim p_{EX}(\\omega)} \\log{\\bigg( \\frac{p_{EX}(\\omega)}{p_{EX}(\\omega) + p_{GZ}(\\omega)}} \\bigg) + \\mathbb{E}_{\\omega \\sim p_{GZ}(\\omega)} \\log{\\bigg(\\frac{p_{GZ}(\\omega)}{p_{EX}(\\omega) + p_{GZ}(\\omega)}\\bigg)} \\\\\n","&= \\int_{\\omega} p_{EX}(\\omega) \\log{\\bigg( \\frac{p_{EX}(\\omega)}{p_{EX}(\\omega) + p_{GZ}(\\omega)}} \\bigg) d\\omega + \\int_{\\omega} p_{GZ}(\\omega) \\log{\\bigg(\\frac{p_{GZ}(\\omega)}{p_{EX}(\\omega) + p_{GZ}(\\omega)}\\bigg)} d\\omega\n","\\end{align*}\n","\n","We will note the Kullback–Leibler (KL) divergences in the above objective function for the generator and encoder.\n","\n","$$\n","D_{KL}(P||Q) = \\int_{x} p(x) \\log\\bigg(\\frac{p(x)}{q(x)}\\bigg) dx\n","$$\n","\n","Recall the definition of a $\\lambda$ divergence.\n","\n","$$\n","D_{\\lambda}(P||Q) = \\lambda D_{KL}(P||\\lambda P + (1 - \\lambda) Q) + (1 - \\lambda) D_{KL}(Q||\\lambda P + (1 - \\lambda) Q)\n","$$\n","\n","If $\\lambda$ takes the value of 0.5 this is then called the Jensen-Shannon (JS) divergence. This divergence is symmetric and non-negative.\n","\n","$$\n","D_{JS}(P||Q) = 0.5 D_{KL}\\bigg(P\\bigg|\\bigg|\\frac{P + Q}{2}\\bigg) + 0.5 D_{KL}\\bigg(Q\\bigg|\\bigg|\\frac{P + Q}{2}\\bigg)\n","$$\n","\n","Keeping this in mind let us take a look again at the objective function of the generator and the encoder.\n","\n","\\begin{align*}\n","J(G(z|\\theta_{G}), E(x|\\theta_{E})) &= \\int_{\\omega} p_{EX}(\\omega) \\log{\\bigg( \\frac{p_{EX}(\\omega)}{p_{EX}(\\omega) + p_{GZ}(\\omega)}} \\bigg) d\\omega + \\int_{\\omega} p_{GZ}(\\omega) \\log{\\bigg(\\frac{p_{GZ}(\\omega)}{p_{EX}(\\omega) + p_{GZ}(\\omega)}\\bigg)} d\\omega \\\\\n","&= \\int_{\\omega} p_{EX}(\\omega) \\log{\\bigg(\\frac{2}{2}\\frac{p_{EX}(\\omega)}{p_{EX}(\\omega) + p_{GZ}(\\omega)}} \\bigg) d\\omega + \\int_{\\omega} p_{GZ}(\\omega) \\log{\\bigg(\\frac{2}{2}\\frac{p_{GZ}(\\omega)}{p_{EX}(\\omega) + p_{GZ}(\\omega)}\\bigg)} d\\omega \\\\\n","&= \\int_{\\omega} p_{EX}(\\omega) \\log{\\bigg(\\frac{1}{2}\\frac{1}{0.5}\\frac{p_{EX}(\\omega)}{p_{EX}(\\omega) + p_{GZ}(\\omega)}} \\bigg) d\\omega + \\int_{\\omega} p_{GZ}(\\omega) \\log{\\bigg(\\frac{1}{2}\\frac{1}{0.5}\\frac{p_{GZ}(\\omega)}{p_{EX}(\\omega) + p_{GZ}(\\omega)}\\bigg)} d\\omega \\\\\n","&= \\int_{\\omega} p_{EX}(\\omega) \\bigg[ \\log(0.5) + \\log{\\bigg(\\frac{p_{EX}(\\omega)}{0.5 (p_{EX}(\\omega) + p_{GZ}(\\omega))}} \\bigg) \\bigg] d\\omega \\\\ &+ \\int_{\\omega} p_{GZ}(\\omega) \\bigg[\\log(0.5) + \\log{\\bigg(\\frac{p_{GZ}(\\omega)}{0.5 (p_{EX}(\\omega) + p_{GZ}(\\omega))}\\bigg) \\bigg] } d\\omega \\\\\n","&= \\log\\bigg(\\frac{1}{4}\\bigg) + \\int_{\\omega} p_{EX}(\\omega) \\bigg[\\log{\\bigg(\\frac{p_{EX}(\\omega)}{0.5 (p_{EX}(\\omega) + p_{GZ}(\\omega))}} \\bigg) \\bigg] d\\omega \\\\ \n","&+ \\int_{\\omega} p_{GZ}(\\omega) \\bigg[\\log{\\bigg(\\frac{p_{GZ}(\\omega)}{0.5 (p_{EX}(\\omega) + p_{GZ}(\\omega))}\\bigg) \\bigg] } d\\omega \\\\\n","&= -\\log(4) + D_{KL}\\bigg(P_{EX}\\bigg|\\bigg|\\frac{P_{EX} + P_{GZ}}{2}\\bigg) + D_{KL}\\bigg(P_{GZ}\\bigg|\\bigg|\\frac{P_{EX} + P_{GZ}}{2}\\bigg) \\\\\n","&= -\\log(4) + 2 \\bigg(0.5 D_{KL}\\bigg(P_{EX}\\bigg|\\bigg|\\frac{P_{EX} + P_{GZ}}{2}\\bigg) + 0.5 D_{KL}\\bigg(P_{GZ}\\bigg|\\bigg|\\frac{P_{EX} + P_{GZ}}{2}\\bigg)\\bigg) \\\\\n","&= -\\log(4) + 2D_{JS}(P_{EX}||P_{GZ}) \n","\\end{align*}\n","\n","It is clear from the objective function of the generator and encoder above that the global minimum value attained is $-\\log(4)$ which occurs when the following holds.\n","\n","$$\n","P_{EX}=P_{GZ}\n","$$\n","\n","When the above holds the Jensen-Shannon divergence, i.e., $D_{JS}(P_{EX}||P_{GZ})$, will be zero. Hence we have shown that the optimal solution is as follows.\n","\n","$$\n","P_{EX}=P_{GZ}\n","$$\n","\n","Given the above result we can prove that the optimal discriminator will be $\\frac{1}{2}$.\n","\n","\\begin{align*}\n","D^\\ast(\\omega|\\theta_{D^\\ast}) &= \\frac{p_{EX}(\\omega)}{p_{EX}(\\omega) + p_{GZ}(\\omega)} \\\\\n"," &= \\frac{p_{EX}(\\omega)}{p_{EX}(\\omega) + p_{EX}(\\omega)} \\\\\n"," &= \\frac{p_{EX}(\\omega)}{2p_{EX}(\\omega)} \\\\\n"," &= \\frac{1}{2} \\\\\n","\\end{align*}\n","\n","### Optimal encoder and generator are inverse functions of each other\n","\n","---\n","\n","At the optimal generator and encoder we can show that the generator and encoder are inverse functions of each other. Recall from earlier the definitions of the generator and the encoder.\n","\n","$$\n","G(z|\\theta_{G}): z \\rightarrow x_{synthetic}\n","$$ \n","\n","$$\n","E(x|\\theta_{E}): x \\rightarrow z\n","$$ \n","\n","At this point the optimal discriminator is $\\frac{1}{2}$, i.e., the discriminator cannot effectively differentiate between real and synthetic data as the synthetic data is realistic. Remember that at this point the likelihood would have been implicitly maximized such that any samples taken from the synthetic distribution should be similar to those taken from the real distribution. In short, if optimality of the generator, encoder and discriminator holds then the synthetic data should look similar, or rather be the same, as the real data. Keeping this important point in mind let's slightly re-write the optimal generator and encoder functions.\n","\n","$$\n","G^\\ast(z|\\theta_{G^\\ast}): z \\rightarrow x\n","$$ \n","\n","$$\n","E^\\ast(x|\\theta_{E^\\ast}): x \\rightarrow z\n","$$ \n","\n","Recall further that the following holds at the optimal generator and encoder.\n","\n","\\begin{align*}\n","P_{EX} &= \\int_{x} p_{X}(x) \\int_{z=E^\\ast(x|\\theta_{E^\\ast})} p_{E^\\ast}(z|x) dz dx \\\\\n","\\end{align*}\n","\n","In the above please note the following; note also that we make the assumption that the generator is not an inverse function of the encoder for providing a proof by contradiction.\n","\n","\\begin{align*}\n","z&=E^\\ast(x|\\theta_{E^\\ast}) \\\\\n","x&\\neq G^\\ast(E^\\ast(x|\\theta_{E^\\ast})|\\theta_{G^\\ast}) \\\\\n","\\end{align*}\n","\n","Recall that optimality condition of the generator and encoder.\n","\n","\\begin{align*}\n","P_{EX} &= P_{GZ} \\\\\n","P_{GZ} &= \\int_{z} p_{Z}(z) \\int_{x=G^\\ast(z|\\theta_{G^\\ast})} p_{G^\\ast}(x|z) dx dz \\\\\n","\\end{align*}\n","\n","In the above please note the following.\n","\n","\\begin{align*}\n","x&=G^\\ast(z|\\theta_{G^\\ast}) \\\\\n","z&=E^\\ast(x|\\theta_{E^\\ast}) \\\\\n","z&=E^\\ast(G^\\ast(z|\\theta_{G^\\ast})|\\theta_{E^\\ast}) \\\\\n","G^\\ast(z|\\theta_{G^\\ast})&=G^\\ast(E^\\ast(G^\\ast(z|\\theta_{G^\\ast})|\\theta_{E^\\ast})|\\theta_{G^\\ast}) \\\\\n","\\end{align*}\n","\n","If optimality holds then the following holds as shown above.\n","\n","$$\n","G^\\ast(z|\\theta_{G^\\ast})=G^\\ast(E^\\ast(G^\\ast(z|\\theta_{G^\\ast})|\\theta_{E^\\ast})|\\theta_{G^\\ast})\n","$$\n","\n","However since we assumed that the generator is not an inverse function of the encoder then the above conditions cannot hold thereby violating the optimality condition.\n","\n","\\begin{align*}\n","x&\\neq G^\\ast(E^\\ast(x|\\theta_{E^\\ast})|\\theta_{G^\\ast}) \\\\\n","G^\\ast(z|\\theta_{G^\\ast})&\\neq G^\\ast(E^\\ast(G^\\ast(z|\\theta_{G^\\ast})|\\theta_{E^\\ast})|\\theta_{G^\\ast}) \\\\\n","\\end{align*}\n","\n","Therefore we have shown by contradiction that under optimality of the generator and encoder the generator is an inverse function of the encoder.\n","\n","\\begin{align*}\n","x&=G^\\ast(E^\\ast(x|\\theta_{E^\\ast})|\\theta_{G^\\ast}) \\\\\n","\\end{align*}\n","\n","The same arguments made above can be shown for the encoder being the inverse of the generator.\n","\n","\\begin{align*}\n","P_{GZ} &= \\int_{z} p_{Z}(z) \\int_{x=G^\\ast(z|\\theta_{G^\\ast})} p_{G^\\ast}(x|z) dx dz \\\\\n","\\end{align*}\n","\n","In the above please note the following; note also that we make the assumption that the encoder is not an inverse function of the generator for providing a proof by contradiction.\n","\n","\\begin{align*}\n","x&=G^\\ast(z|\\theta_{G^\\ast}) \\\\\n","z&\\neq E^\\ast(G^\\ast(z|\\theta_{G^\\ast})|\\theta_{E^\\ast}) \\\\\n","\\end{align*}\n","\n","Recall that optimality condition of the generator and encoder.\n","\n","\\begin{align*}\n","P_{EX} &= P_{GZ} \\\\\n","P_{EX} &= \\int_{x} p_{X}(x) \\int_{z=E^\\ast(x|\\theta_{E^\\ast})} p_{E^\\ast}(z|x) dz dx \\\\\n","\\end{align*}\n","\n","In the above please note the following.\n","\n","\\begin{align*}\n","z&=E^\\ast(x|\\theta_{E^\\ast}) \\\\\n","x&=G^\\ast(z|\\theta_{G^\\ast}) \\\\\n","x&=G^\\ast(E^\\ast(x|\\theta_{E^\\ast})|\\theta_{G^\\ast}) \\\\\n","E^\\ast(x|\\theta_{E^\\ast})&=E^\\ast(G^\\ast(E^\\ast(x|\\theta_{E^\\ast})|\\theta_{G^\\ast})|\\theta_{E^\\ast}) \\\\\n","\\end{align*}\n","\n","If optimality holds then the following holds as shown above.\n","\n","$$\n","E^\\ast(x|\\theta_{E^\\ast})=E^\\ast(G^\\ast(E^\\ast(x|\\theta_{E^\\ast})|\\theta_{G^\\ast})|\\theta_{E^\\ast})\n","$$\n","\n","However since we assumed that the encoder is not an inverse function of the generator then the above conditions cannot hold thereby violating the optimality condition.\n","\n","\\begin{align*}\n","z&\\neq E^\\ast(G^\\ast(z|\\theta_{G^\\ast})|\\theta_{E^\\ast}) \\\\\n","E^\\ast(x|\\theta_{E^\\ast})&\\neq E^\\ast(G^\\ast(E^\\ast(x|\\theta_{E^\\ast})|\\theta_{G^\\ast})|\\theta_{E^\\ast}) \\\\\n","\\end{align*}\n","\n","Therefore we have shown by contradiction that under optimality of the generator and encoder the encoder is an inverse function of the generator.\n","\n","\\begin{align*}\n","z&= E^\\ast(G^\\ast(z|\\theta_{G^\\ast})|\\theta_{E^\\ast}) \\\\\n","\\end{align*}\n","\n","Therefore we have shown that the optimal encoder and generator are inverse functions of each other via proof by contradiction: If they were not inverse functions of each other then it would violate the optimality condition for the encoder and generator, i.e.,  $P_{EX} = P_{GZ}$.\n","\n"]},{"cell_type":"code","metadata":{"id":"oBfUPf5ffbhe","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":357},"outputId":"8ae9557d-a58a-4e5e-dd69-b68f02a59020","executionInfo":{"status":"ok","timestamp":1562722670935,"user_tz":-60,"elapsed":1868,"user":{"displayName":"Cloud Machine","photoUrl":"","userId":"03543453517966682716"}}},"source":["from pandas_datareader import data as pdr\n","import fix_yahoo_finance as yf\n","yf.pdr_override()\n","df_full = pdr.get_data_yahoo([\"AAPL\",\"GOOGL\",\"MSFT\",\"INTC\",\"BOX\"], start=\"2016-01-01\", end=\"2017-01-01\").reset_index()\n","df_full.head()"],"execution_count":1,"outputs":[{"output_type":"stream","text":["[*********************100%***********************]  5 of 5 downloaded\n"],"name":"stdout"},{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead tr th {\n","        text-align: left;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr>\n","      <th></th>\n","      <th>Date</th>\n","      <th colspan=\"5\" halign=\"left\">Open</th>\n","      <th colspan=\"5\" halign=\"left\">High</th>\n","      <th colspan=\"5\" halign=\"left\">Low</th>\n","      <th colspan=\"5\" halign=\"left\">Close</th>\n","      <th colspan=\"5\" halign=\"left\">Adj Close</th>\n","      <th colspan=\"5\" halign=\"left\">Volume</th>\n","    </tr>\n","    <tr>\n","      <th></th>\n","      <th></th>\n","      <th>AAPL</th>\n","      <th>BOX</th>\n","      <th>GOOGL</th>\n","      <th>INTC</th>\n","      <th>MSFT</th>\n","      <th>AAPL</th>\n","      <th>BOX</th>\n","      <th>GOOGL</th>\n","      <th>INTC</th>\n","      <th>MSFT</th>\n","      <th>AAPL</th>\n","      <th>BOX</th>\n","      <th>GOOGL</th>\n","      <th>INTC</th>\n","      <th>MSFT</th>\n","      <th>AAPL</th>\n","      <th>BOX</th>\n","      <th>GOOGL</th>\n","      <th>INTC</th>\n","      <th>MSFT</th>\n","      <th>AAPL</th>\n","      <th>BOX</th>\n","      <th>GOOGL</th>\n","      <th>INTC</th>\n","      <th>MSFT</th>\n","      <th>AAPL</th>\n","      <th>BOX</th>\n","      <th>GOOGL</th>\n","      <th>INTC</th>\n","      <th>MSFT</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>2016-01-04</td>\n","      <td>102.610001</td>\n","      <td>13.46</td>\n","      <td>762.200012</td>\n","      <td>33.880001</td>\n","      <td>54.320000</td>\n","      <td>105.370003</td>\n","      <td>14.36</td>\n","      <td>762.200012</td>\n","      <td>34.009998</td>\n","      <td>54.799999</td>\n","      <td>102.000000</td>\n","      <td>13.40</td>\n","      <td>747.539978</td>\n","      <td>33.459999</td>\n","      <td>53.389999</td>\n","      <td>105.349998</td>\n","      <td>14.36</td>\n","      <td>759.440002</td>\n","      <td>33.990002</td>\n","      <td>54.799999</td>\n","      <td>99.117409</td>\n","      <td>14.36</td>\n","      <td>759.440002</td>\n","      <td>30.827848</td>\n","      <td>50.877312</td>\n","      <td>67649400</td>\n","      <td>935200</td>\n","      <td>3369100</td>\n","      <td>27882200</td>\n","      <td>53778000</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2016-01-05</td>\n","      <td>105.750000</td>\n","      <td>14.25</td>\n","      <td>764.099976</td>\n","      <td>33.959999</td>\n","      <td>54.930000</td>\n","      <td>105.849998</td>\n","      <td>14.29</td>\n","      <td>769.200012</td>\n","      <td>34.000000</td>\n","      <td>55.389999</td>\n","      <td>102.410004</td>\n","      <td>13.27</td>\n","      <td>755.650024</td>\n","      <td>33.529999</td>\n","      <td>54.540001</td>\n","      <td>102.709999</td>\n","      <td>13.32</td>\n","      <td>761.530029</td>\n","      <td>33.830002</td>\n","      <td>55.049999</td>\n","      <td>96.633583</td>\n","      <td>13.32</td>\n","      <td>761.530029</td>\n","      <td>30.682732</td>\n","      <td>51.109421</td>\n","      <td>55791000</td>\n","      <td>1173300</td>\n","      <td>2260800</td>\n","      <td>16709500</td>\n","      <td>34079700</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2016-01-06</td>\n","      <td>100.559998</td>\n","      <td>13.18</td>\n","      <td>750.369995</td>\n","      <td>33.250000</td>\n","      <td>54.320000</td>\n","      <td>102.370003</td>\n","      <td>13.26</td>\n","      <td>765.729980</td>\n","      <td>33.520000</td>\n","      <td>54.400002</td>\n","      <td>99.870003</td>\n","      <td>12.51</td>\n","      <td>748.000000</td>\n","      <td>32.799999</td>\n","      <td>53.639999</td>\n","      <td>100.699997</td>\n","      <td>12.69</td>\n","      <td>759.330017</td>\n","      <td>33.080002</td>\n","      <td>54.049999</td>\n","      <td>94.742485</td>\n","      <td>12.69</td>\n","      <td>759.330017</td>\n","      <td>30.002504</td>\n","      <td>50.181000</td>\n","      <td>68457400</td>\n","      <td>1143000</td>\n","      <td>2410300</td>\n","      <td>25491300</td>\n","      <td>39518900</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>2016-01-07</td>\n","      <td>98.680000</td>\n","      <td>12.41</td>\n","      <td>746.489990</td>\n","      <td>32.279999</td>\n","      <td>52.700001</td>\n","      <td>100.129997</td>\n","      <td>12.51</td>\n","      <td>755.309998</td>\n","      <td>33.009998</td>\n","      <td>53.490002</td>\n","      <td>96.430000</td>\n","      <td>12.00</td>\n","      <td>735.280029</td>\n","      <td>31.840000</td>\n","      <td>52.070000</td>\n","      <td>96.449997</td>\n","      <td>12.17</td>\n","      <td>741.000000</td>\n","      <td>31.840000</td>\n","      <td>52.169998</td>\n","      <td>90.743942</td>\n","      <td>12.17</td>\n","      <td>741.000000</td>\n","      <td>28.877863</td>\n","      <td>48.435574</td>\n","      <td>81094400</td>\n","      <td>993100</td>\n","      <td>3156600</td>\n","      <td>37680500</td>\n","      <td>56564900</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2016-01-08</td>\n","      <td>98.550003</td>\n","      <td>12.31</td>\n","      <td>747.799988</td>\n","      <td>32.090000</td>\n","      <td>52.369999</td>\n","      <td>99.110001</td>\n","      <td>12.47</td>\n","      <td>750.119995</td>\n","      <td>32.220001</td>\n","      <td>53.279999</td>\n","      <td>96.760002</td>\n","      <td>11.56</td>\n","      <td>728.919983</td>\n","      <td>31.430000</td>\n","      <td>52.150002</td>\n","      <td>96.959999</td>\n","      <td>11.61</td>\n","      <td>730.909973</td>\n","      <td>31.510000</td>\n","      <td>52.330002</td>\n","      <td>91.223770</td>\n","      <td>11.61</td>\n","      <td>730.909973</td>\n","      <td>28.578564</td>\n","      <td>48.584122</td>\n","      <td>70798000</td>\n","      <td>831100</td>\n","      <td>2375300</td>\n","      <td>29953800</td>\n","      <td>48754000</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["        Date        Open         ...   Volume                    \n","                    AAPL    BOX  ...    GOOGL      INTC      MSFT\n","0 2016-01-04  102.610001  13.46  ...  3369100  27882200  53778000\n","1 2016-01-05  105.750000  14.25  ...  2260800  16709500  34079700\n","2 2016-01-06  100.559998  13.18  ...  2410300  25491300  39518900\n","3 2016-01-07   98.680000  12.41  ...  3156600  37680500  56564900\n","4 2016-01-08   98.550003  12.31  ...  2375300  29953800  48754000\n","\n","[5 rows x 31 columns]"]},"metadata":{"tags":[]},"execution_count":1}]},{"cell_type":"code","metadata":{"id":"x8T_8o2ggGmz","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":233},"outputId":"22955d36-1eb5-48d1-cc8e-fcacc6fa1082","executionInfo":{"status":"ok","timestamp":1562722673353,"user_tz":-60,"elapsed":839,"user":{"displayName":"Cloud Machine","photoUrl":"","userId":"03543453517966682716"}}},"source":["df_full = df_full.set_index('Date')\n","df_full = df_full['Adj Close']\n","df_full = df_full.pct_change().ffill().bfill()\n","df_full.head()"],"execution_count":2,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>AAPL</th>\n","      <th>BOX</th>\n","      <th>GOOGL</th>\n","      <th>INTC</th>\n","      <th>MSFT</th>\n","    </tr>\n","    <tr>\n","      <th>Date</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>2016-01-04</th>\n","      <td>-0.025059</td>\n","      <td>-0.072423</td>\n","      <td>0.002752</td>\n","      <td>-0.004707</td>\n","      <td>0.004562</td>\n","    </tr>\n","    <tr>\n","      <th>2016-01-05</th>\n","      <td>-0.025059</td>\n","      <td>-0.072423</td>\n","      <td>0.002752</td>\n","      <td>-0.004707</td>\n","      <td>0.004562</td>\n","    </tr>\n","    <tr>\n","      <th>2016-01-06</th>\n","      <td>-0.019570</td>\n","      <td>-0.047297</td>\n","      <td>-0.002889</td>\n","      <td>-0.022170</td>\n","      <td>-0.018165</td>\n","    </tr>\n","    <tr>\n","      <th>2016-01-07</th>\n","      <td>-0.042204</td>\n","      <td>-0.040977</td>\n","      <td>-0.024140</td>\n","      <td>-0.037485</td>\n","      <td>-0.034783</td>\n","    </tr>\n","    <tr>\n","      <th>2016-01-08</th>\n","      <td>0.005288</td>\n","      <td>-0.046015</td>\n","      <td>-0.013617</td>\n","      <td>-0.010364</td>\n","      <td>0.003067</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                AAPL       BOX     GOOGL      INTC      MSFT\n","Date                                                        \n","2016-01-04 -0.025059 -0.072423  0.002752 -0.004707  0.004562\n","2016-01-05 -0.025059 -0.072423  0.002752 -0.004707  0.004562\n","2016-01-06 -0.019570 -0.047297 -0.002889 -0.022170 -0.018165\n","2016-01-07 -0.042204 -0.040977 -0.024140 -0.037485 -0.034783\n","2016-01-08  0.005288 -0.046015 -0.013617 -0.010364  0.003067"]},"metadata":{"tags":[]},"execution_count":2}]},{"cell_type":"code","metadata":{"id":"f4ELTGC7dK8k","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":609},"outputId":"18cb84d8-6574-465b-f112-112de57404bb","executionInfo":{"status":"ok","timestamp":1562722679977,"user_tz":-60,"elapsed":5627,"user":{"displayName":"Cloud Machine","photoUrl":"","userId":"03543453517966682716"}}},"source":["from PIL import Image\n","\n","from six.moves import range\n","\n","import os\n","import math\n","import inspect\n","import sys\n","import importlib\n","\n","import numpy as np\n","\n","import pandas as pd\n","\n","from sklearn.base import BaseEstimator, TransformerMixin\n","from sklearn import linear_model\n","from sklearn.pipeline import Pipeline\n","from sklearn.preprocessing import MinMaxScaler, LabelBinarizer\n","from sklearn.metrics import roc_auc_score\n","from sklearn.model_selection import train_test_split\n","\n","import keras\n","from keras import backend as bkend\n","from keras import layers\n","from keras.layers import Input, Dense, BatchNormalization, Dropout, Flatten, convolutional, pooling, Reshape, concatenate\n","from keras.layers.advanced_activations import LeakyReLU\n","from keras.layers.convolutional import UpSampling2D, Conv2D\n","from keras import metrics\n","from keras.models import Model\n","from keras.optimizers import Adam, RMSprop\n","from keras.utils.generic_utils import Progbar\n","\n","import tensorflow as tf\n","from tensorflow.python.client import device_lib\n","\n","import matplotlib.pyplot as plt\n","\n","from plotnine import *\n","import plotnine\n","\n","get_ipython().magic(\"matplotlib inline\")\n","\n","os.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\n","importlib.reload(bkend)\n","\n","print(device_lib.list_local_devices())\n","\n","ret_data = df_full.copy()\n","mean = ret_data.apply(func=np.mean, axis=0)\n","std = ret_data.apply(func=np.std, axis=0)\n","ret_data -= mean\n","ret_data /= std"],"execution_count":3,"outputs":[{"output_type":"stream","text":["Using TensorFlow backend.\n","Using TensorFlow backend.\n"],"name":"stderr"},{"output_type":"stream","text":["[name: \"/device:CPU:0\"\n","device_type: \"CPU\"\n","memory_limit: 268435456\n","locality {\n","}\n","incarnation: 9899822543429676215\n",", name: \"/device:XLA_CPU:0\"\n","device_type: \"XLA_CPU\"\n","memory_limit: 17179869184\n","locality {\n","}\n","incarnation: 7926420924738207938\n","physical_device_desc: \"device: XLA_CPU device\"\n",", name: \"/device:XLA_GPU:0\"\n","device_type: \"XLA_GPU\"\n","memory_limit: 17179869184\n","locality {\n","}\n","incarnation: 15166282187119119058\n","physical_device_desc: \"device: XLA_GPU device\"\n",", name: \"/device:GPU:0\"\n","device_type: \"GPU\"\n","memory_limit: 11326753997\n","locality {\n","  bus_id: 1\n","  links {\n","  }\n","}\n","incarnation: 12995622532743248017\n","physical_device_desc: \"device: 0, name: Tesla K80, pci bus id: 0000:00:04.0, compute capability: 3.7\"\n","]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"3yhEbBjOdK8w","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"a2f8dd3d-aec3-46c3-8f46-6b750a46baa7","executionInfo":{"status":"ok","timestamp":1562723045711,"user_tz":-60,"elapsed":353206,"user":{"displayName":"Cloud Machine","photoUrl":"","userId":"03543453517966682716"}}},"source":["# License\n","# Copyright 2018 Hamaad Musharaf Shah\n","# Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with the License. You may obtain a copy of the License at\n","# http://www.apache.org/licenses/LICENSE-2.0\n","# Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the specific language governing permissions and limitations under the License.\n","\n","# BiGAN implementation inspired from here: https://github.com/eriklindernoren/Keras-GAN\n","# I believe I can re-write it slightly differently. On my to-do list.\n","class BiGAN(BaseEstimator,\n","            TransformerMixin):\n","    def __init__(self,\n","                 z_size=None,\n","                 iterations=None,\n","                 batch_size=None):\n","        args, _, _, values = inspect.getargvalues(inspect.currentframe())\n","        values.pop(\"self\")\n","        \n","        for arg, val in values.items():\n","            setattr(self, arg, val)\n","            \n","        # Build the discriminator.\n","        self.discriminator = self.build_discriminator()\n","        self.discriminator.compile(optimizer=RMSprop(lr=0.0002, \n","                                                     clipvalue=1.0,\n","                                                     decay=1e-8),\n","                                   loss=\"binary_crossentropy\",\n","                                   metrics=[\"accuracy\"])\n","\n","        # Build the generator to fool the discriminator.\n","        # Freeze the discriminator here.\n","        self.discriminator.trainable = False\n","        self.generator = self.build_generator()\n","        self.encoder = self.build_encoder()\n","        \n","        noise = Input(shape=(self.z_size, ))\n","        generated_data = self.generator(noise)\n","        fake = self.discriminator([noise, generated_data])\n","\n","        real_data = Input(shape=(5,))\n","        encoding = self.encoder(real_data)\n","        valid = self.discriminator([encoding, real_data])\n","\n","        # Set up and compile the combined model.\n","        # Trains generator to fool the discriminator.\n","        self.bigan_generator = Model([noise, real_data], [fake, valid])\n","        self.bigan_generator.compile(loss=[\"binary_crossentropy\", \"binary_crossentropy\"],\n","                                     optimizer=RMSprop(lr=0.0004, \n","                                                       clipvalue=1.0,\n","                                                       decay=1e-8))\n"," \n","    def fit(self,\n","            X,\n","            y=None):\n","        num_train = X.shape[0]\n","        start = 0\n","        \n","        # Adversarial ground truths.\n","        valid = np.ones((self.batch_size, 1)) \n","        fake = np.zeros((self.batch_size, 1))        \n","        \n","        for step in range(self.iterations):\n","            # Generate a new batch of noise...\n","            noise = np.random.uniform(low=-1.0, high=1.0, size=(self.batch_size, self.z_size))\n","            # ...and generate a batch of synthetic returns data.\n","            generated_data = self.generator.predict(noise)\n","            \n","            # Get a batch of real returns data...\n","            stop = start + self.batch_size\n","            real_batch = X[start:stop]\n","            # ...and encode them.\n","            encoding = self.encoder.predict(real_batch)\n","\n","            # Train the discriminator.\n","            d_loss_real = self.discriminator.train_on_batch([encoding, real_batch], valid)\n","            d_loss_fake = self.discriminator.train_on_batch([noise, generated_data], fake)\n","            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n","\n","            # Train the generator.\n","            g_loss = self.bigan_generator.train_on_batch([noise, real_batch], [valid, fake])\n","            \n","            start += self.batch_size\n","            if start > num_train - self.batch_size:\n","                start = 0\n","            \n","            if step % 100 == 0:\n","                # Plot the progress.\n","                print(\"[Discriminator loss: %f, Discriminator accuracy: %.2f%%] [Generator loss: %f]\" % (d_loss[0], 100 * d_loss[1], g_loss[0]))\n","                \n","        return self\n","\n","    def transform(self,\n","                  X):\n","        return self.feature_extractor.predict(X)\n","\n","    def build_encoder(self):\n","        encoder_input = Input(shape=(5,))\n","\n","        encoder_model = Dense(units=100)(encoder_input)\n","        encoder_model = LeakyReLU(alpha=0.2)(encoder_model)\n","        encoder_model = BatchNormalization()(encoder_model)\n","        encoder_model = Dense(units=100)(encoder_model)\n","        encoder_model = LeakyReLU(alpha=0.2)(encoder_model)\n","        \n","        encoder_output = Dense(units=self.z_size, activation=\"tanh\")(encoder_model)\n","        \n","        self.feature_extractor = Model(encoder_input, encoder_output)\n","        \n","        return Model(encoder_input, encoder_output)\n","    \n","    def build_generator(self):\n","        # We will map z, a latent vector, to continuous returns data space (..., 5).\n","        latent = Input(shape=(self.z_size,))\n","\n","        # This produces a (..., 100) shaped tensor.\n","        generator_model = Dense(units=100, activation=\"elu\")(latent)\n","        generator_model = BatchNormalization()(generator_model)\n","        generator_model = Dense(units=100, activation=\"elu\")(generator_model)\n","        generator_model = BatchNormalization()(generator_model)\n","\n","        generator_output = Dense(units=5, activation=\"linear\")(generator_model)\n","        \n","        return Model(latent, generator_output)\n","    \n","    def build_discriminator(self):\n","        z = Input(shape=(self.z_size,))\n","        ret_data = Input(shape=(5,))\n","        discriminator_inputs = concatenate([z, ret_data], axis=1)\n","\n","        discriminator_model = Dense(units=100)(discriminator_inputs)\n","        discriminator_model = LeakyReLU(alpha=0.2)(discriminator_model)\n","        discriminator_model = Dropout(rate=0.5)(discriminator_model)\n","\n","        discriminator_output = Dense(units=1, activation=\"sigmoid\")(discriminator_model)\n","        \n","        return Model([z, ret_data], discriminator_output)\n","\n","z_size = 10\n","bigan = BiGAN(z_size=z_size,\n","              batch_size=100,\n","              iterations=10000)\n","\n","bigan.fit(X=ret_data)\n","\n","n_sim = 1000\n","noise = np.random.uniform(low=-1.0, high=1.0, size=(n_sim, z_size))\n","x = np.zeros(shape=(n_sim, 5))\n","x_mean = np.zeros(shape=n_sim)\n","for i, xi in enumerate(noise):\n","    x[i, :] = (bigan.generator.predict(x=np.array([xi]))[0] * std) + mean\n","    x_mean[i] = np.average(a=x[i, :])\n","\n","act_mean = np.zeros(shape=ret_data.shape[0])\n","for i in range(ret_data.shape[0]):\n","    act_mean[i] = np.average(a=(ret_data.iloc[i] * std) + mean)\n","    \n","plotnine.options.figure_size = (12, 9)\n","plot = ggplot(pd.melt(pd.concat([pd.DataFrame(x_mean, columns=[\"BiGAN Portfolio Returns Distribution\"]).reset_index(drop=True),\n","                                 pd.DataFrame(act_mean, columns=[\"Actual Portfolio Returns Distribution\"]).reset_index(drop=True)],\n","                                axis=1))) + \\\n","geom_density(aes(x=\"value\",\n","                 fill=\"factor(variable)\"), \n","             alpha=0.5,\n","             color=\"black\") + \\\n","geom_point(aes(x=\"value\",\n","               y=0,\n","               fill=\"factor(variable)\"), \n","           alpha=0.5, \n","           color=\"black\") + \\\n","xlab(\"Portfolio returns\") + \\\n","ylab(\"Density\") + \\\n","ggtitle(\"Trained Bidirectional Generative Adversarial Network (BiGAN) Portfolio Returns\") + \\\n","theme_matplotlib()\n","plot.save(filename=\"trained_bigan_sampler.png\")\n","\n","untrained_bigan = BiGAN(z_size=z_size,\n","                        batch_size=100,\n","                        iterations=10000)\n","\n","untrained_x = np.zeros(shape=(n_sim, 5))\n","untrained_x_mean = np.zeros(shape=n_sim)\n","for i, xi in enumerate(noise):\n","    untrained_x[i, :] = (untrained_bigan.generator.predict(x=np.array([xi]))[0] * std) + mean\n","    untrained_x_mean[i] = np.average(a=untrained_x[i, :])\n","\n","plotnine.options.figure_size = (12, 9)\n","plot = ggplot(pd.melt(pd.concat([pd.DataFrame(untrained_x_mean, columns=[\"BiGAN Portfolio Returns Distribution\"]).reset_index(drop=True),\n","                                 pd.DataFrame(act_mean, columns=[\"Actual Portfolio Returns Distribution\"]).reset_index(drop=True)],\n","                                axis=1))) + \\\n","geom_density(aes(x=\"value\",\n","                 fill=\"factor(variable)\"), \n","             alpha=0.5,\n","             color=\"black\") + \\\n","geom_point(aes(x=\"value\",\n","               y=0,\n","               fill=\"factor(variable)\"), \n","           alpha=0.5, \n","           color=\"black\") + \\\n","xlab(\"Portfolio returns\") + \\\n","ylab(\"Density\") + \\\n","ggtitle(\"Untrained Bidirectional Generative Adversarial Network (BiGAN) Portfolio Returns\") + \\\n","theme_matplotlib()\n","plot.save(filename=\"untrained_bigan_sampler.png\")\n","\n","print(\"The VaR at 1%% estimate given by the BiGAN: %.2f%%\" % (100 * np.percentile(a=x_mean, axis=0, q=1)))"],"execution_count":4,"outputs":[{"output_type":"stream","text":["WARNING: Logging before flag parsing goes to stderr.\n","W0710 01:38:12.955640 140333317891968 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","W0710 01:38:12.961388 140333317891968 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","W0710 01:38:12.976481 140333317891968 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","W0710 01:38:12.995869 140333317891968 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n","\n","W0710 01:38:13.008757 140333317891968 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n","W0710 01:38:13.058076 140333317891968 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n","\n","W0710 01:38:13.065927 140333317891968 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3376: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n","W0710 01:38:13.072712 140333317891968 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/nn_impl.py:180: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n","  'Discrepancy between trainable weights and collected trainable'\n"],"name":"stderr"},{"output_type":"stream","text":["[Discriminator loss: 0.694480, Discriminator accuracy: 51.50%] [Generator loss: 1.638100]\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/keras/engine/training.py:490: UserWarning: Discrepancy between trainable weights and collected trainable weights, did you set `model.trainable` without calling `model.compile` after ?\n","  'Discrepancy between trainable weights and collected trainable'\n"],"name":"stderr"},{"output_type":"stream","text":["[Discriminator loss: 0.820894, Discriminator accuracy: 38.00%] [Generator loss: 1.255132]\n","[Discriminator loss: 0.764588, Discriminator accuracy: 47.00%] [Generator loss: 1.353739]\n","[Discriminator loss: 0.722767, Discriminator accuracy: 46.50%] [Generator loss: 1.425732]\n","[Discriminator loss: 0.736330, Discriminator accuracy: 46.00%] [Generator loss: 1.371265]\n","[Discriminator loss: 0.685728, Discriminator accuracy: 51.50%] [Generator loss: 1.413202]\n","[Discriminator loss: 0.700723, Discriminator accuracy: 48.50%] [Generator loss: 1.361335]\n","[Discriminator loss: 0.697648, Discriminator accuracy: 51.50%] [Generator loss: 1.382622]\n","[Discriminator loss: 0.727550, Discriminator accuracy: 46.50%] [Generator loss: 1.411593]\n","[Discriminator loss: 0.723220, Discriminator accuracy: 42.50%] [Generator loss: 1.399636]\n","[Discriminator loss: 0.707107, Discriminator accuracy: 50.00%] [Generator loss: 1.369449]\n","[Discriminator loss: 0.717697, Discriminator accuracy: 51.00%] [Generator loss: 1.426095]\n","[Discriminator loss: 0.716321, Discriminator accuracy: 41.50%] [Generator loss: 1.349723]\n","[Discriminator loss: 0.693872, Discriminator accuracy: 49.00%] [Generator loss: 1.418867]\n","[Discriminator loss: 0.699259, Discriminator accuracy: 50.50%] [Generator loss: 1.387594]\n","[Discriminator loss: 0.712057, Discriminator accuracy: 48.50%] [Generator loss: 1.426285]\n","[Discriminator loss: 0.718446, Discriminator accuracy: 49.50%] [Generator loss: 1.369619]\n","[Discriminator loss: 0.718529, Discriminator accuracy: 44.00%] [Generator loss: 1.406084]\n","[Discriminator loss: 0.698402, Discriminator accuracy: 52.00%] [Generator loss: 1.428973]\n","[Discriminator loss: 0.709787, Discriminator accuracy: 44.00%] [Generator loss: 1.394464]\n","[Discriminator loss: 0.715934, Discriminator accuracy: 42.00%] [Generator loss: 1.375506]\n","[Discriminator loss: 0.701266, Discriminator accuracy: 50.00%] [Generator loss: 1.430146]\n","[Discriminator loss: 0.711380, Discriminator accuracy: 48.00%] [Generator loss: 1.390056]\n","[Discriminator loss: 0.705719, Discriminator accuracy: 44.50%] [Generator loss: 1.391239]\n","[Discriminator loss: 0.699308, Discriminator accuracy: 51.50%] [Generator loss: 1.381802]\n","[Discriminator loss: 0.704094, Discriminator accuracy: 48.00%] [Generator loss: 1.361536]\n","[Discriminator loss: 0.696207, Discriminator accuracy: 49.00%] [Generator loss: 1.410610]\n","[Discriminator loss: 0.714435, Discriminator accuracy: 42.50%] [Generator loss: 1.426813]\n","[Discriminator loss: 0.703304, Discriminator accuracy: 52.00%] [Generator loss: 1.406240]\n","[Discriminator loss: 0.710916, Discriminator accuracy: 45.50%] [Generator loss: 1.388495]\n","[Discriminator loss: 0.711842, Discriminator accuracy: 44.00%] [Generator loss: 1.384573]\n","[Discriminator loss: 0.707178, Discriminator accuracy: 39.50%] [Generator loss: 1.403783]\n","[Discriminator loss: 0.698714, Discriminator accuracy: 46.50%] [Generator loss: 1.399393]\n","[Discriminator loss: 0.697314, Discriminator accuracy: 46.50%] [Generator loss: 1.390302]\n","[Discriminator loss: 0.695575, Discriminator accuracy: 54.00%] [Generator loss: 1.402238]\n","[Discriminator loss: 0.691281, Discriminator accuracy: 53.00%] [Generator loss: 1.396981]\n","[Discriminator loss: 0.704785, Discriminator accuracy: 46.50%] [Generator loss: 1.388526]\n","[Discriminator loss: 0.708141, Discriminator accuracy: 48.00%] [Generator loss: 1.389314]\n","[Discriminator loss: 0.699586, Discriminator accuracy: 46.50%] [Generator loss: 1.390453]\n","[Discriminator loss: 0.703702, Discriminator accuracy: 47.50%] [Generator loss: 1.398654]\n","[Discriminator loss: 0.704184, Discriminator accuracy: 50.50%] [Generator loss: 1.382457]\n","[Discriminator loss: 0.691429, Discriminator accuracy: 57.50%] [Generator loss: 1.385882]\n","[Discriminator loss: 0.694499, Discriminator accuracy: 49.00%] [Generator loss: 1.392426]\n","[Discriminator loss: 0.699069, Discriminator accuracy: 43.00%] [Generator loss: 1.397136]\n","[Discriminator loss: 0.700058, Discriminator accuracy: 47.00%] [Generator loss: 1.388679]\n","[Discriminator loss: 0.697485, Discriminator accuracy: 49.50%] [Generator loss: 1.397111]\n","[Discriminator loss: 0.688321, Discriminator accuracy: 55.50%] [Generator loss: 1.387590]\n","[Discriminator loss: 0.706209, Discriminator accuracy: 40.50%] [Generator loss: 1.391977]\n","[Discriminator loss: 0.695143, Discriminator accuracy: 51.50%] [Generator loss: 1.406426]\n","[Discriminator loss: 0.707353, Discriminator accuracy: 44.00%] [Generator loss: 1.390247]\n","[Discriminator loss: 0.702379, Discriminator accuracy: 46.50%] [Generator loss: 1.390239]\n","[Discriminator loss: 0.694605, Discriminator accuracy: 51.00%] [Generator loss: 1.380049]\n","[Discriminator loss: 0.701186, Discriminator accuracy: 48.50%] [Generator loss: 1.385317]\n","[Discriminator loss: 0.686742, Discriminator accuracy: 51.50%] [Generator loss: 1.374313]\n","[Discriminator loss: 0.698514, Discriminator accuracy: 47.50%] [Generator loss: 1.383512]\n","[Discriminator loss: 0.696300, Discriminator accuracy: 49.00%] [Generator loss: 1.390834]\n","[Discriminator loss: 0.694625, Discriminator accuracy: 50.00%] [Generator loss: 1.396312]\n","[Discriminator loss: 0.693530, Discriminator accuracy: 50.50%] [Generator loss: 1.393397]\n","[Discriminator loss: 0.694981, Discriminator accuracy: 45.00%] [Generator loss: 1.394621]\n","[Discriminator loss: 0.695575, Discriminator accuracy: 51.00%] [Generator loss: 1.401938]\n","[Discriminator loss: 0.696090, Discriminator accuracy: 52.00%] [Generator loss: 1.378446]\n","[Discriminator loss: 0.700429, Discriminator accuracy: 51.00%] [Generator loss: 1.392141]\n","[Discriminator loss: 0.695619, Discriminator accuracy: 53.00%] [Generator loss: 1.371353]\n","[Discriminator loss: 0.695577, Discriminator accuracy: 45.00%] [Generator loss: 1.388199]\n","[Discriminator loss: 0.697183, Discriminator accuracy: 50.00%] [Generator loss: 1.391295]\n","[Discriminator loss: 0.695353, Discriminator accuracy: 48.50%] [Generator loss: 1.394301]\n","[Discriminator loss: 0.698930, Discriminator accuracy: 46.50%] [Generator loss: 1.383818]\n","[Discriminator loss: 0.692643, Discriminator accuracy: 48.00%] [Generator loss: 1.382235]\n","[Discriminator loss: 0.692131, Discriminator accuracy: 51.00%] [Generator loss: 1.404670]\n","[Discriminator loss: 0.696904, Discriminator accuracy: 47.50%] [Generator loss: 1.381827]\n","[Discriminator loss: 0.692821, Discriminator accuracy: 51.00%] [Generator loss: 1.383774]\n","[Discriminator loss: 0.696194, Discriminator accuracy: 49.00%] [Generator loss: 1.380543]\n","[Discriminator loss: 0.693133, Discriminator accuracy: 50.50%] [Generator loss: 1.390041]\n","[Discriminator loss: 0.695360, Discriminator accuracy: 49.00%] [Generator loss: 1.389367]\n","[Discriminator loss: 0.692453, Discriminator accuracy: 50.50%] [Generator loss: 1.393209]\n","[Discriminator loss: 0.692748, Discriminator accuracy: 48.50%] [Generator loss: 1.393089]\n","[Discriminator loss: 0.695122, Discriminator accuracy: 42.50%] [Generator loss: 1.382837]\n","[Discriminator loss: 0.693922, Discriminator accuracy: 50.50%] [Generator loss: 1.384848]\n","[Discriminator loss: 0.699196, Discriminator accuracy: 38.00%] [Generator loss: 1.387701]\n","[Discriminator loss: 0.700199, Discriminator accuracy: 43.00%] [Generator loss: 1.389089]\n","[Discriminator loss: 0.699073, Discriminator accuracy: 49.00%] [Generator loss: 1.388985]\n","[Discriminator loss: 0.690591, Discriminator accuracy: 52.50%] [Generator loss: 1.383827]\n","[Discriminator loss: 0.695597, Discriminator accuracy: 47.50%] [Generator loss: 1.393499]\n","[Discriminator loss: 0.695966, Discriminator accuracy: 49.00%] [Generator loss: 1.379528]\n","[Discriminator loss: 0.692775, Discriminator accuracy: 48.00%] [Generator loss: 1.386126]\n","[Discriminator loss: 0.693981, Discriminator accuracy: 48.00%] [Generator loss: 1.395420]\n","[Discriminator loss: 0.697229, Discriminator accuracy: 44.00%] [Generator loss: 1.382479]\n","[Discriminator loss: 0.694995, Discriminator accuracy: 47.50%] [Generator loss: 1.389204]\n","[Discriminator loss: 0.696558, Discriminator accuracy: 51.00%] [Generator loss: 1.372826]\n","[Discriminator loss: 0.697747, Discriminator accuracy: 46.50%] [Generator loss: 1.388700]\n","[Discriminator loss: 0.692865, Discriminator accuracy: 50.50%] [Generator loss: 1.382701]\n","[Discriminator loss: 0.696017, Discriminator accuracy: 45.00%] [Generator loss: 1.387204]\n","[Discriminator loss: 0.693281, Discriminator accuracy: 54.50%] [Generator loss: 1.384244]\n","[Discriminator loss: 0.696144, Discriminator accuracy: 50.00%] [Generator loss: 1.387947]\n","[Discriminator loss: 0.695327, Discriminator accuracy: 47.50%] [Generator loss: 1.383882]\n","[Discriminator loss: 0.692904, Discriminator accuracy: 49.50%] [Generator loss: 1.387581]\n","[Discriminator loss: 0.693210, Discriminator accuracy: 45.00%] [Generator loss: 1.384629]\n","[Discriminator loss: 0.691599, Discriminator accuracy: 55.00%] [Generator loss: 1.389023]\n","[Discriminator loss: 0.690320, Discriminator accuracy: 52.50%] [Generator loss: 1.396613]\n","[Discriminator loss: 0.692871, Discriminator accuracy: 51.00%] [Generator loss: 1.390090]\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/plotnine/ggplot.py:706: UserWarning: Saving 12 x 9 in image.\n","  from_inches(height, units), units))\n","/usr/local/lib/python3.6/dist-packages/plotnine/ggplot.py:707: UserWarning: Filename: trained_bigan_sampler.png\n","  warn('Filename: {}'.format(filename))\n","/usr/local/lib/python3.6/dist-packages/plotnine/layer.py:517: MatplotlibDeprecationWarning: isinstance(..., numbers.Number)\n","  return not cbook.iterable(value) and (cbook.is_numlike(value) or\n","/usr/local/lib/python3.6/dist-packages/plotnine/layer.py:360: UserWarning: stat_density : Removed 748 rows containing non-finite values.\n","  data = self.stat.compute_layer(data, params, layout)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py:855: MatplotlibDeprecationWarning: \n","examples.directory is deprecated; in the future, examples will be found relative to the 'datapath' directory.\n","  \"found relative to the 'datapath' directory.\".format(key))\n","/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py:846: MatplotlibDeprecationWarning: \n","The text.latex.unicode rcparam was deprecated in Matplotlib 2.2 and will be removed in 3.1.\n","  \"2.2\", name=key, obj_type=\"rcparam\", addendum=addendum)\n","/usr/local/lib/python3.6/dist-packages/plotnine/layer.py:449: UserWarning: geom_point : Removed 748 rows containing missing values.\n","  self.data = self.geom.handle_na(self.data)\n","/usr/local/lib/python3.6/dist-packages/plotnine/ggplot.py:706: UserWarning: Saving 12 x 9 in image.\n","  from_inches(height, units), units))\n","/usr/local/lib/python3.6/dist-packages/plotnine/ggplot.py:707: UserWarning: Filename: untrained_bigan_sampler.png\n","  warn('Filename: {}'.format(filename))\n","/usr/local/lib/python3.6/dist-packages/plotnine/layer.py:517: MatplotlibDeprecationWarning: isinstance(..., numbers.Number)\n","  return not cbook.iterable(value) and (cbook.is_numlike(value) or\n","/usr/local/lib/python3.6/dist-packages/plotnine/layer.py:360: UserWarning: stat_density : Removed 748 rows containing non-finite values.\n","  data = self.stat.compute_layer(data, params, layout)\n","/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py:855: MatplotlibDeprecationWarning: \n","examples.directory is deprecated; in the future, examples will be found relative to the 'datapath' directory.\n","  \"found relative to the 'datapath' directory.\".format(key))\n","/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py:846: MatplotlibDeprecationWarning: \n","The text.latex.unicode rcparam was deprecated in Matplotlib 2.2 and will be removed in 3.1.\n","  \"2.2\", name=key, obj_type=\"rcparam\", addendum=addendum)\n","/usr/local/lib/python3.6/dist-packages/plotnine/layer.py:449: UserWarning: geom_point : Removed 748 rows containing missing values.\n","  self.data = self.geom.handle_na(self.data)\n"],"name":"stderr"},{"output_type":"stream","text":["The VaR at 1% estimate given by the BiGAN: -2.16%\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"5OMymQyTdK83","colab_type":"text"},"source":["## Conclusion\n","\n","Before we conclude the article let's have a look at the portfolio returns distribution sampled from an untrained BiGAN. \n","\n","![](untrained_bigan_sampler.png)\n","\n","It is clear from the above graph that the untrained BiGAN's sampled portfolio returns distribution is remarkably different from the actual portfolio returns distribution. This is, as we can imagine, to be expected.\n","\n","Contrast this with a trained BiGAN: The following graph will clearly show the value of GAN type models for market risk management as we have arrived at this learnt portfolio returns distribution without having to rely on a possibly incorrect assumption with regards to the actual portfolio returns distribution such as a multidimensional Gaussian distribution.\n","\n","Note that we perhaps should use an evolutionary algorithm or a reinforcement learner to automatically learn the appropriate GAN or BiGAN architecture: Perhaps that shall be a topic for a future article.\n","\n","![](trained_bigan_sampler.png)\n","\n","Finally we update the VaR estimate table for using different market risk management methods as below. We can see that the VaR estimate provided by the BiGAN is similar, if not exactly the same, to the ones provided by the other market risk management methods. This provides us with a good sanity check with regards to using the BiGAN for market risk management in that it provides competitive results with respect to well established existing market risk management methods.\n","\n","| VaR Method    | 1% VaR | \n","| :------------- |-------------:|\n","| Variance-covariance | -1.37% | \n","| Historical simulation | -1.65%  |\n","| Monte Carlo simulation | -1.43%  |\n","| Bidirectional Generative Adversarial Network | -2.16%  |\n","\n","The portfolio of 5 stocks we had to work with was not particularly complicated compared to potentially having portfolios where we might have derivatives or other portfolio components. Arriving at the correct portfolio returns distribution of a potentially complicated portfolio is a problem that has been shown can be solved via deep learning specifically the BiGAN. This result can be useful for market risk management and any other different problem space where we need to generate samples from a potentially complex, and perhaps unknown, distribution. \n","\n","There will potentially be a follow up article of mine where we look at a complicated backtesting scenario, i.e., validating that market risk management VaR type estimates provided by BiGAN is appropriate for future portfolio returns distributions that we have not seen, and perhaps using more complicated portfolios. The aim of this article of mine was to clearly show that a trained BiGAN can be used for market risk management VaR estimation for a given portfolio."]},{"cell_type":"markdown","metadata":{"heading_collapsed":true,"id":"4mNAEwT0dK8_","colab_type":"text"},"source":["## References\n","\n","1. Goodfellow, I., Bengio, Y. and Courville A. (2016). Deep Learning (MIT Press).\n","2. Geron, A. (2017). Hands-On Machine Learning with Scikit-Learn & Tensorflow (O'Reilly).\n","3. Kingma, D. P., and Welling M. (2014). Auto-Encoding Variational Bayes (https://arxiv.org/abs/1312.6114).\n","4. http://scikit-learn.org/stable/#\n","5. https://towardsdatascience.com/learning-rate-schedules-and-adaptive-learning-rate-methods-for-deep-learning-2c8f433990d1\n","6. https://stackoverflow.com/questions/42177658/how-to-switch-backend-with-keras-from-tensorflow-to-theano\n","7. https://blog.keras.io/building-autoencoders-in-keras.html\n","8. https://keras.io\n","9. Chollet, F. (2018). Deep Learning with Python (Manning).\n","10. Hull, John C. (2010). Risk Management and Financial Institutions (Pearson).\n","11. https://towardsdatascience.com/automatic-feature-engineering-using-deep-learning-and-bayesian-inference-application-to-computer-7b2bb8dc7351\n","12. https://towardsdatascience.com/automatic-feature-engineering-using-generative-adversarial-networks-8e24b3c16bf3\n","13. Donahue, J., Krähenbühl, P. and Darrell, T. (2017). Adversarial Feature Learning (https://arxiv.org/pdf/1605.09782).\n","14. https://github.com/eriklindernoren/Keras-GAN"]}]}